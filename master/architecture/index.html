<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Red Hat Quay Architecture</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.4"/><meta name="description" content="Red Hat Quay Architecture"/><link rel="next" href="#arch-intro" title="Chapter 1. Red Hat Quay overview"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm45401799687376"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Quay</span> <span class="productnumber">3.7</span></div><div><h1 class="title">Red Hat Quay Architecture</h1></div><div><h2 class="subtitle">Red Hat Quay Architecture</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm45401782315840">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Red Hat Quay Architecture
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="chapter"><a href="#arch-intro">1. Red Hat Quay overview</a></span><ul><li><span class="section"><a href="#arch-intro-scalability">1.1. Scalability and high availability (HA)</a></span></li><li><span class="section"><a href="#arch-intro-content-distribution">1.2. Content distribution</a></span></li><li><span class="section"><a href="#arch-intro-build-automation">1.3. Build automation</a></span></li><li><span class="section"><a href="#arch-intro-integration">1.4. Integration</a></span><ul><li><span class="section"><a href="#rest_api">1.4.1. REST API</a></span></li></ul></li><li><span class="section"><a href="#arch-intro-recent-features">1.5. Recently added features</a></span></li><li><span class="section"><a href="#arch-intro-other-features">1.6. Other features</a></span></li><li><span class="section"><a href="#arch-intro-security">1.7. Security</a></span></li></ul></li><li><span class="chapter"><a href="#arch-prereqs">2. Red Hat Quay prerequisites</a></span><ul><li><span class="section"><a href="#core-prereqs-storage">2.1. Image storage backend</a></span><ul><li><span class="section"><a href="#supported_image_storage_types">2.1.1. Supported image storage types</a></span></li></ul></li><li><span class="section"><a href="#core-prereqs-db">2.2. Database backend</a></span></li><li><span class="section"><a href="#core-prereqs-redis">2.3. Redis</a></span></li></ul></li><li><span class="chapter"><a href="#red_hat_quay_infrastructure">3. Red Hat Quay infrastructure</a></span><ul><li><span class="section"><a href="#running_red_hat_quay_on_standalone_hosts">3.1. Running Red Hat Quay on standalone hosts</a></span></li><li><span class="section"><a href="#running_red_hat_quay_on_openshift">3.2. Running Red Hat Quay on OpenShift</a></span></li><li><span class="section"><a href="#integrating_standalone_red_hat_quay_with_openshift">3.3. Integrating standalone Red Hat Quay with OpenShift</a></span></li><li><span class="section"><a href="#arch-mirror-registry">3.4. Mirror registry for Red Hat OpenShift</a></span></li><li><span class="section"><a href="#core-distinct-registries">3.5. Single versus multiple registries</a></span></li></ul></li><li><span class="chapter"><a href="#sample-quay-on-prem-intro">4. Deploying Red Hat Quay on-prem</a></span><ul><li><span class="section"><a href="#core-example-deployment">4.1. Red Hat Quay example deployments</a></span></li><li><span class="section"><a href="#deployment-topology">4.2. Red Hat Quay deployment topology</a></span></li><li><span class="section"><a href="#deployment-topology-with-storage-proxy">4.3. Red Hat Quay deployment topology with storage proxy</a></span></li></ul></li><li><span class="chapter"><a href="#deploying_red_hat_quay_on_public_cloud">5. Deploying Red Hat Quay on public cloud</a></span><ul><li><span class="section"><a href="#running_red_hat_quay_on_aws">5.1. Running Red Hat Quay on AWS</a></span></li><li><span class="section"><a href="#running_red_hat_quay_on_microsoft_azure">5.2. Running Red Hat Quay on Microsoft Azure</a></span></li></ul></li><li><span class="chapter"><a href="#security-intro">6. Red Hat Quay security overview</a></span><ul><li><span class="section"><a href="#clair-intro">6.1. Red Hat Quay vulnerability scanning using Clair</a></span><ul><li><span class="section"><a href="#clair-analyses">6.1.1. Understanding Clair analyses</a></span></li><li><span class="section"><a href="#clairv4-intro">6.1.2. Clair v4</a></span></li><li><span class="section"><a href="#clairv4-arch">6.1.3. Clair v4 architecture</a></span></li><li><span class="section"><a href="#clairv2-to-v4">6.1.4. Migrating from Clair v2 to Clair v4</a></span></li><li><span class="section"><a href="#clairv4-limitations">6.1.5. Clair v4 limitations</a></span></li><li><span class="section"><a href="#clairv4-air-gapped">6.1.6. Air-gapped Clair v4</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#content-distrib-intro">7. Content distribution with Red Hat Quay</a></span><ul><li><span class="section"><a href="#mirroring-intro">7.1. Repository mirroring</a></span><ul><li><span class="section"><a href="#mirroring-using">7.1.1. Using repository mirroring</a></span></li><li><span class="section"><a href="#mirroring-recommend">7.1.2. Repository mirroring recommendations</a></span></li><li><span class="section"><a href="#mirroring-events">7.1.3. Event notifications for mirroring</a></span></li><li><span class="section"><a href="#mirroring-api-intro">7.1.4. Mirroring API</a></span></li></ul></li><li><span class="section"><a href="#georepl-intro">7.2. Geo-replication</a></span><ul><li><span class="section"><a href="#geo_replication_features">7.2.1. Geo-replication features</a></span></li><li><span class="section"><a href="#georepl-prereqs">7.2.2. Geo-replication requirements and constraints</a></span></li><li><span class="section"><a href="#georepl-arch-standalone">7.2.3. Geo-replication using standalone Red Hat Quay</a></span></li><li><span class="section"><a href="#georepl-arch-operator">7.2.4. Geo-replication using the Red Hat Quay Operator</a></span></li><li><span class="section"><a href="#georepl-mixed-storage">7.2.5. Mixed storage for geo-replication</a></span></li></ul></li><li><span class="section"><a href="#mirroring-versus-georepl">7.3. Repository mirroring versus geo-replication</a></span></li><li><span class="section"><a href="#airgap-intro">7.4. Air-gapped / disconnected deployments</a></span><ul><li><span class="section"><a href="#airgap-clair">7.4.1. Using Clair in air-gapped environments</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#sizing-intro">8. Red Hat Quay sizing and subscriptions</a></span><ul><li><span class="section"><a href="#sizing-sample">8.1. Red Hat Quay sample sizings</a></span></li><li><span class="section"><a href="#subscription-intro">8.2. Red Hat Quay subscription information</a></span></li><li><span class="section"><a href="#quay-internal-registry-intro">8.3. Using Red Hat Quay with or without internal registry</a></span></li></ul></li></ul></div><section class="chapter" id="arch-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Red Hat Quay overview</h1></div></div></div><p>
			Red Hat Quay is a trusted, open source container registry platform that runs everywhere, but runs best on Red Hat OpenShift. It scales without limits, from a developer laptop to a container host or Kubernetes, and can be deployed on-prem or on public cloud. Red Hat Quay provides global governance and security controls, with features including image vulnerability scanning, access controls, geo-replication and repository mirroring.
		</p><p>
			<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_features.png" alt="Quay features"/></span>
		</p><p>
			This guide provides an insight into architectural patterns to use when deploying Red Hat Quay. It contains sizing guidance and deployment prerequisites, along with best practices for ensuring high availability for your Red Hat Quay registry.
		</p><section class="section" id="arch-intro-scalability"><div class="titlepage"><div><div><h2 class="title">1.1. Scalability and high availability (HA)</h2></div></div></div><p>
				The code base for the private Red Hat Quay offering is substantially the same as that used for <a class="link" href="https::/quay.io">quay.io</a>, the highly available container image registry hosted by Red Hat which provides a multi-tenant SaaS solution. As a result, you can be confident that Red Hat Quay can deliver at scale with high availability, whether you deploy on-prem or on public cloud.
			</p></section><section class="section" id="arch-intro-content-distribution"><div class="titlepage"><div><div><h2 class="title">1.2. Content distribution</h2></div></div></div><p>
				Content distribution features in Red Hat Quay include:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Repository mirroring</span></dt><dd>
							Red Hat Quay repository mirroring lets you mirror images from external container registries (or another local registry) into your Red Hat Quay cluster. Using repository mirroring, you can synchronize images to Red Hat Quay based on repository names and tags.
						</dd><dt><span class="term">Geo-replication</span></dt><dd>
							Red Hat Quay geo-replication allows multiple, geographically distributed Quay deployments to work as a single registry from the perspective of a client or user. It significantly improves push and pull performance in a globally-distributed Red Hat Quay setup. Image data is asynchronously replicated in the background with transparent failover / redirect for clients.
						</dd><dt><span class="term">Deployment in disconnected or air-gapped environments</span></dt><dd><p class="simpara">
							Red Hat Quay can be deployed in a disconnected environment in two ways:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Red Hat Quay and Clair connected to the internet, with an air-gapped OpenShift cluster accessing the Quay registry through an explicit, white-listed hole in the firewall.
								</li><li class="listitem">
									Red Hat Quay and Clair running inside the firewall, with image and CVE data transferred to the target system using offline media. The data is exported from a separate Quay and Clair deployment that is connected to the internet.
								</li></ul></div></dd></dl></div></section><section class="section" id="arch-intro-build-automation"><div class="titlepage"><div><div><h2 class="title">1.3. Build automation</h2></div></div></div><p>
				Red Hat Quay supports building Dockerfiles using a set of worker nodes on OpenShift or Kubernetes. Build triggers, such as GitHub webhooks, can be configured to automatically build new versions of your repositories when new code is committed.
			</p><p>
				Prior to Red Hat Quay 3.7, Quay ran podman commands in virtual machines launched by pods. Running builds on virtual platforms requires enabling nested virtualization, which is not featured in Red Hat Enterprise Linux or OpenShift Container Platform. As a result, builds had to run on bare-metal clusters, which is an inefficient use of resources.
			</p><p>
				With Red Hat Quay 3.7, the bare-metal constraint required to run builds has been removed by adding an additional build option which does not contain the virtual machine layer. As a result, builds can be run on virtualized platforms. Backwards compatibility to run previous build configurations is also available.
			</p></section><section class="section" id="arch-intro-integration"><div class="titlepage"><div><div><h2 class="title">1.4. Integration</h2></div></div></div><p>
				Integration with popular source code management and versioning systems like GitHub, GitLab or BitBucket allows Red Hat Quay to continuously build and serve your containerized software.
			</p><section class="section" id="rest_api"><div class="titlepage"><div><div><h3 class="title">1.4.1. REST API</h3></div></div></div><p>
					Red Hat Quay provides a full OAuth 2, RESTful API that:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Is available from endpoints of each Red Hat Quay instance from the URL <a class="link" href="https://&lt;yourquayhost&gt;/api/v1">https://&lt;yourquayhost&gt;/api/v1</a>
						</li><li class="listitem">
							Lets you connect to endpoints, via a browser, to get, delete, post, and put Red Hat Quay settings by enabling the Swagger UI
						</li><li class="listitem">
							Can be accessed by applications that make API calls and use OAuth tokens
						</li><li class="listitem">
							Sends and receives data as JSON
						</li></ul></div></section></section><section class="section" id="arch-intro-recent-features"><div class="titlepage"><div><div><h2 class="title">1.5. Recently added features</h2></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Storage Quota on Organizations</span></dt><dd>
							Control and contain storage growth of your container registry with reporting and enforcement.
						</dd><dt><span class="term">Transparent pull-thru cache proxy (Tech preview)</span></dt><dd>
							Use Red Hat Quay as a transparent cache for other registry for improved performance and resiliency.
						</dd><dt><span class="term">Geo-replication with the Operator</span></dt><dd>
							Deploy a geographically dispersed container registry across two or more OpenShift clusters.
						</dd><dt><span class="term">Red Hat Quay container builds on OpenShift</span></dt><dd>
							Build your container images right inside Quay running on top of OpenShift.
						</dd></dl></div></section><section class="section" id="arch-intro-other-features"><div class="titlepage"><div><div><h2 class="title">1.6. Other features</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Full standards / spec support (Docker v2-2)
					</li><li class="listitem">
						Long-term protocol support
					</li><li class="listitem">
						OCI compatibility through test suite compliance
					</li><li class="listitem">
						Enterprise grade support
					</li><li class="listitem">
						Regular updates
					</li></ul></div></section><section class="section" id="arch-intro-security"><div class="titlepage"><div><div><h2 class="title">1.7. Security</h2></div></div></div><p>
				Red Hat Quay is built for real enterprise use cases where content governance and security are two major focus areas. Red Hat Quay content governance and security includes built-in vulnerability scanning via Clair.
			</p><p>
				Clair is an open source tool developed by CoreOS for Quay that generates analyses of vulnerabilities in application containers, which currently includes Open Container Initiative (OCI) and Docker images. Clients that use the Clair API to index their container images can then match their images against known vulnerabilities.
			</p></section></section><section class="chapter" id="arch-prereqs"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Red Hat Quay prerequisites</h1></div></div></div><p>
			Before deploying Red Hat Quay, you need to provision image storage, a database, and Redis.
		</p><section class="section" id="core-prereqs-storage"><div class="titlepage"><div><div><h2 class="title">2.1. Image storage backend</h2></div></div></div><p>
				Red Hat Quay stores all binary blobs in its storage backend.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Local storage and NFS</span></dt><dd>
							While Red Hat Quay can work with local storage and NFS, this should only be used for proof of concept or test setups, as the durability of the binary blobs cannot be guaranteed.
						</dd><dt><span class="term">HA storage setup</span></dt><dd><p class="simpara">
							For a Red Hat Quay HA deployment, you must provide HA image storage, for example:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Red Hat OpenShift Data Foundation</strong></span>, previously known as Red Hat OpenShift Container Storage, is software-defined storage for containers. Engineered as the data and storage services platform for Red Hat OpenShift, Red Hat OpenShift Data Foundation helps teams develop and deploy applications quickly and efficiently across clouds. More information can be found at <a class="link" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation">https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation</a>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Ceph Object Gateway</strong></span> (also called RADOS Gateway) is an example of a storage solution that can provide the the object storage needed by Red Hat Quay. Detailed instructions on how to use Ceph storage as a highly available storage backend can be found in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_-_high_availability/preparing_for_red_hat_quay_high_availability#set_up_ceph">Quay High Availability Guide</a>. Further information on Red Hat Ceph Storage and HA setups can be found in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/pdf/architecture_guide/Red_Hat_Ceph_Storage-3-Architecture_Guide-en-US.pdf">Red Hat Ceph Storage Architecture Guide</a>
								</li></ul></div></dd><dt><span class="term">Geo-replication</span></dt><dd>
							Local storage cannot be used for geo-replication, so a supported on-prem or cloud-based object storage solution must be deployed. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Quay instance, and will then be replicated, in the background, to the other storage engines. This requires the image storage to be accessible from all regions.
						</dd></dl></div><section class="section" id="supported_image_storage_types"><div class="titlepage"><div><div><h3 class="title">2.1.1. Supported image storage types</h3></div></div></div><p>
					Red Hat Quay supports the following on-prem storage types:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Ceph Rados RGW
						</li><li class="listitem">
							OpenStack Swift
						</li><li class="listitem">
							RHODF 4 (via NooBaa)
						</li></ul></div><p>
					Red Hat Quay supports the following public cloud storage types:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							AWS S3
						</li><li class="listitem">
							Google Cloud Storage
						</li><li class="listitem">
							Azure Blob Storage
						</li></ul></div></section></section><section class="section" id="core-prereqs-db"><div class="titlepage"><div><div><h2 class="title">2.2. Database backend</h2></div></div></div><p>
				Red Hat Quay stores most of its configuration and all metadata and logs inside its database backend, although logs can be pushed to ElasticSearch if required. PostgreSQL is the preferred database backend since it can be used for both Quay and Clair.
			</p><p>
				Since the Red Hat Quay 3.6 release, using MySQL/MariaDB as the database backend for Red Hat Quay is deprecated and support will eventually be removed. Until then, MySQL is still supported as per the <a class="link" href="https://access.redhat.com/articles/4067991">support matrix</a> but will not receive additional features or explicit testing coverage. The Red Hat Quay Operator only supports PostgreSQL as a managed database since Red Hat Quay 3.4. External MySQL/MariaDB databases can still be leveraged (setting the database to <code class="literal">unmanaged</code> in the Operator in the process) until support is removed.
			</p><p>
				Deploying Red Hat Quay in a highly available (HA) configuration requires that your database is provisioned for high availablity. If Red Hat Quay is running on public cloud infrastructure, it is recommended that you use the PostgreSQL services provided by your cloud provider.
			</p><p>
				Geo-replication requires a single, shared database that is accessible from all regions.
			</p></section><section class="section" id="core-prereqs-redis"><div class="titlepage"><div><div><h2 class="title">2.3. Redis</h2></div></div></div><p>
				Red Hat Quay stores builder logs inside a Redis cache. The data stored is ephemeral in nature and as such, Redis does not need to be HA even though it is stateful. If Redis does fail, you will only lose access to build logs. You can use a Redis image from the Red Hat Software Collections or from any other source you prefer.
			</p></section></section><section class="chapter" id="red_hat_quay_infrastructure"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Red Hat Quay infrastructure</h1></div></div></div><p>
			Red Hat Quay runs on any physical or virtual infrastructure, both on-premise or public cloud. Deployments range from simple to massively scaled, including:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					All-in-one setup on a developer laptop
				</li><li class="listitem">
					Highly available on Virtual Machines or on OpenShift
				</li><li class="listitem">
					Geographically dispersed across multiple availability zones and regions
				</li></ul></div><section class="section" id="running_red_hat_quay_on_standalone_hosts"><div class="titlepage"><div><div><h2 class="title">3.1. Running Red Hat Quay on standalone hosts</h2></div></div></div><p>
				Standalone deployment is a manual process, but it can easily be automated by the customer, for example, using Ansible. All standalone hosts require valid RHEL subscriptions.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Proof-of-concept deployment</span></dt><dd>
							Red Hat Quay runs on a machine with image storage, containerized database, Redis, and optionally, Clair security scanning (scanning only works with object storage).
						</dd><dt><span class="term">Highly available setups</span></dt><dd><p class="simpara">
							Red Hat Quay and Clair run in containers across multiple hosts, using <code class="literal">systemd</code> to ensure restart on failure/reboot.
						</p><p class="simpara">
							High availability setups on standalone hosts require customer-provided load balancers, either low-level TCP load balancers or application load balancers capable of terminating TLS.
						</p></dd></dl></div></section><section class="section" id="running_red_hat_quay_on_openshift"><div class="titlepage"><div><div><h2 class="title">3.2. Running Red Hat Quay on OpenShift</h2></div></div></div><p>
				The Red Hat Quay Operator for OpenShift provides the following features:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Automated deployment and Day 2 management of Red Hat Quaywith customization options
					</li><li class="listitem">
						Management of Red Hat Quay and all its dependencies
					</li><li class="listitem">
						Automated scaling and updates
					</li><li class="listitem">
						Integration with existing OpenShift processes like GitOps, monitoring, alerting, logging
					</li><li class="listitem">
						Provision of out-of-the-box object storage with limited availability, backed by the Multi-Cloud Object Gateway (NooBaa), as part of the ODF Operator (no additional subscription required)
					</li><li class="listitem">
						Scaled-out, high availability object storage provided by the ODF Operator (additional subscription required)
					</li></ul></div><p>
				Red Hat Quay can run on OpenShift infrastructure nodes, meaning no further subscriptions are required. Benefits of running Red Hat Quay on OpenShift include:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Zero to Hero:</strong></span> Simplified deployment of Red Hat Quay and associated components means that you can start using the product immediately
					</li><li class="listitem">
						<span class="strong strong"><strong>Scalability:</strong></span> Leverage cluster compute capacity to manage demand via automated scaling, based on actual load
					</li><li class="listitem">
						<span class="strong strong"><strong>Simplified Networking:</strong></span> Automated provisioning of load balancers and traffic ingress secured via HTTPS using OpenShift TLS certificates and Routes
					</li><li class="listitem">
						<span class="strong strong"><strong>Declarative configuration management:</strong></span> Configurations stored in CustomResource objects for GitOps-friendly lifecycle management
					</li><li class="listitem">
						<span class="strong strong"><strong>Repeatability:</strong></span> Consistency regardless of the number of replicas of Red Hat Quay / Clair
					</li><li class="listitem">
						<span class="strong strong"><strong>OpenShift integration:</strong></span> Additional services to leverage OpenShift Monitoring and Alerting facilities to manage multiple Quay deployments on a single cluster
					</li></ul></div></section><section class="section" id="integrating_standalone_red_hat_quay_with_openshift"><div class="titlepage"><div><div><h2 class="title">3.3. Integrating standalone Red Hat Quay with OpenShift</h2></div></div></div><p>
				While the Red Hat Quay Operator ensures seamless deployment and management of Red Hat Quay running on OpenShift, it is also possible to run Red Hat Quay in standalone mode and then serve content to one or many OpenShift clusters, wherever they are running.
			</p><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_deployment_ex2.png" alt="Integrating standalone Quay with OpenShift"/></span>
			</p><p>
				A number of Operators are available to help integrate standalone Quay with OpenShift:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Red Hat Quay Cluster Security Operator</span></dt><dd>
							Relays Quay vulnerability scanning results into the OpenShift Console
						</dd><dt><span class="term">Red Hat Quay Bridge Operator</span></dt><dd>
							Ensures seamless integration and user experience for using Red Hat Quay with OpenShift in conjunction with OpenShift Builds and ImageStreams
						</dd></dl></div></section><section class="section" id="arch-mirror-registry"><div class="titlepage"><div><div><h2 class="title">3.4. Mirror registry for Red Hat OpenShift</h2></div></div></div><p>
				The mirror registry for Red Hat OpenShift is small-scale version of Red Hat Quay that you can use as a target for mirroring the required container images of OpenShift Container Platform for disconnected installations.
			</p><p>
				For disconnected deployments of Red Hat OpenShift, a container registry is required to carry out the installation of the clusters. To run a production-grade registry service on such a cluster, you must create a separate registry deployment to install the first cluster. The <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> addresses this need and is included in every OpenShift subscription. It is available for download on the <a class="link" href="https://console.redhat.com/openshift/downloads#tool-mirror-registry">OpenShift console <span class="strong strong"><strong>Downloads</strong></span></a> page.
			</p><p>
				The <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> allows users to install a small-scale version of Red Hat Quay and its required components using the <code class="literal">mirror-registry</code> command line interface (CLI) tool. The <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> is deployed automatically with pre-configured local storage and a local database. It also includes auto-generated user credentials and access permissions with a single set of inputs and no additional configuration choices to get started.
			</p><p>
				The <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> provides a pre-determined network configuration and reports deployed component credentials and access URLs upon success. A limited set of optional configuration inputs like fully qualified domain name (FQDN) services, superuser name and password, and custom TLS certificates are also provided. This provides users with a container registry so that they can easily create an offline mirror of all Red Hat OpenShift release content when running Red Hat OpenShift in restricted network environments.
			</p><p>
				The <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> is limited to hosting images that are required to install a disconnected Red Hat OpenShift cluster, such as Release images or Red Hat Operator images. It uses local storage and content built by customers should not be hosted by the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span>.
			</p><p>
				Unlike Red Hat Quay, the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> is not a highly-available registry and only local file system storage is supported. Using the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> with more than one cluster is discouraged, because multiple clusters can create a single point of failure when updating your cluster fleet. It is advised to leverage the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> to install a cluster that can host a production-grade, highly-available registry such as Red Hat Quay, which can serve Red Hat OpenShift content to other clusters.
			</p><p>
				More information is available at <a class="link" href="https://docs.openshift.com/container-platform/4.10/installing/disconnected_install/installing-mirroring-creating-registry.html">Creating a mirror registry with mirror registry for Red Hat OpenShift</a>.
			</p></section><section class="section" id="core-distinct-registries"><div class="titlepage"><div><div><h2 class="title">3.5. Single versus multiple registries</h2></div></div></div><p>
				Many users consider running multiple, distinct registries. The preferred approach with Red Hat Quay is to have a single, shared registry:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you want a clear separation between development and production images, or a clear separation by content origin, for example, keeping third-party images distinct from internal ones, you can use organizations and repositories Red Hat Quay, combined with role-based access control (RBAC) to achieve the desired separation.
					</li><li class="listitem">
						Given that the image registry is a critical component in an enterprise, you may be tempted to use distinct deployments to test upgrades of the registry software to newer versions. The Red Hat Quay Operator updates the registry for patch releases as well as minor or major updates. This means that any complicated procedures are automated and, as a result, there is no requirement for you to provision multiple instances of the registry to test the upgrade.
					</li><li class="listitem">
						With Red Hat Quay, there is no need to have a separate registry for each cluster you deploy. Red Hat Quay is proven to work at scale at <a class="link" href="https://quay.io">quay.io</a>, and can serve content to thousands of clusters.
					</li><li class="listitem">
						Even if you have deployments in multiple datacenters, you can still use a single Red Hat Quay instance to serve content to multiple physically-close datacenters, or use the HA functionality with load balancers to stretch across datacenters. Alternatively, you can use the Red Hat Quay geo-replication feature to stretch across physically distant datacenters. This requires the provisioning of a global load balancer or DNS-based geo-aware load balancing.
					</li><li class="listitem">
						One scenario where it may be appropriate to run multiple distinct registries, is when you want to specify different configuration for each registry.
					</li></ul></div><p>
				In summary, running a shared registry helps you to save storage, infrastructure and operational costs but a dedicated registry may be needed in very specific circumstances.
			</p></section></section><section class="chapter" id="sample-quay-on-prem-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Deploying Red Hat Quay on-prem</h1></div></div></div><p>
			The following image shows examples for on-premise configuration, including:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Standalone proof of concept
				</li><li class="listitem">
					Highly available deployment on multiple hosts
				</li><li class="listitem">
					Deployment on OpenShift cluster using Operator
				</li></ul></div><p>
			<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_on-premises_config.png" alt="On-prem sample configuration"/></span>
		</p><section class="section" id="core-example-deployment"><div class="titlepage"><div><div><h2 class="title">4.1. Red Hat Quay example deployments</h2></div></div></div><p>
				The following image shows three possible deployments for Red Hat Quay:
			</p><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_deployment_ex1.png" alt="Red Hat Quay deployment example"/></span>
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Proof of concept</span></dt><dd>
							Running Red Hat Quay, Clair and mirroring on a single node, with local image storage and local database
						</dd><dt><span class="term">Single datacenter</span></dt><dd>
							Running highly available Red Hat Quay, Clair and mirroring, on multiple nodes, with HA database and image storage
						</dd><dt><span class="term">Multiple datacenters</span></dt><dd>
							Running highly available Red Hat Quay, Clair and mirroring, on multiple nodes in multiple datacenters, with HA database and image storage
						</dd></dl></div></section><section class="section" id="deployment-topology"><div class="titlepage"><div><div><h2 class="title">4.2. Red Hat Quay deployment topology</h2></div></div></div><p>
				The following image provides a high level overview of a Red Hat Quay deployment topology.
			</p><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_deploy_topology.png" alt="Red Hat Quay deployment topology"/></span>
			</p><p>
				In this deployment, all pushes, UI, and API requests come in via public Quay endpoints. Pulls are served directly from <code class="literal">object storage</code>.
			</p></section><section class="section" id="deployment-topology-with-storage-proxy"><div class="titlepage"><div><div><h2 class="title">4.3. Red Hat Quay deployment topology with storage proxy</h2></div></div></div><p>
				The following image provides a high level overview of a Red Hat Quay deployment topology with a storage proxy configured.
			</p><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_deploy_topology_storage.png" alt="Red Hat Quay deployment topology with storage proxy"/></span>
			</p><p>
				With a storage proxy configured, all traffic passes through the public Quay endpoint.
			</p></section></section><section class="chapter" id="deploying_red_hat_quay_on_public_cloud"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Deploying Red Hat Quay on public cloud</h1></div></div></div><p>
			Red Hat Quay can run on public clouds, either in standalone mode or where OpenShift itself has been deployed on public cloud. A full list of tested and supported configurations can be found in the Red Hat Quay Tested Integrations Matrix at <a class="link" href="https://access.redhat.com/articles/4067991">https://access.redhat.com/articles/4067991</a>.
		</p><p>
			<span class="strong strong"><strong>Recommendation:</strong></span> If Red Hat Quay is running on public cloud, then you should use the public cloud services for Red Hat Quay backend services to ensure proper HA and scalability.
		</p><section class="section" id="running_red_hat_quay_on_aws"><div class="titlepage"><div><div><h2 class="title">5.1. Running Red Hat Quay on AWS</h2></div></div></div><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_on_AWS.png" alt="Red Hat Quay on AWS"/></span>
			</p><p>
				If Red Hat Quay is running on AWS, you can use
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						AWS Elastic Load Balancer
					</li><li class="listitem">
						AWS S3 (hot) blob storage
					</li><li class="listitem">
						AWS RDS database
					</li><li class="listitem">
						AWS ElastiCache Redis
					</li><li class="listitem">
						EC2 VMs recommendation: M3.Large or M4.XLarge
					</li></ul></div></section><section class="section" id="running_red_hat_quay_on_microsoft_azure"><div class="titlepage"><div><div><h2 class="title">5.2. Running Red Hat Quay on Microsoft Azure</h2></div></div></div><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_on_Azure.png" alt="Red Hat Quay on Microsoft Azure"/></span>
			</p><p>
				If Red Hat Quay is running on Microsoft Azure, you can use:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Azure managed services such as HA PostgreSQL
					</li><li class="listitem">
						Azure Blob Storage must be hot storage (not Azure Cool Blob Storage)
					</li><li class="listitem">
						Azure Cache for Redis
					</li></ul></div></section></section><section class="chapter" id="security-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Red Hat Quay security overview</h1></div></div></div><p>
			Red Hat Quay is built for real enterprise use cases where content governance and security are two major focus areas. Red Hat Quay content governance and security includes a built-in vulnerability scanning via Clair.
		</p><p>
			Clair is an open source tool developed by CoreOS for Quay that generates analyses of vulnerabilities in application containers, which currently includes Open Container Initiative (OCI) and Docker images. Clients that use the Clair API to index their container images can then match their images against known vulnerabilities.
		</p><p>
			Clair supports the extraction of contents and assignment of vulnerabilities from the following official base containers:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Ubuntu Linux
				</li><li class="listitem">
					Debian Linux
				</li><li class="listitem">
					Red Hat Enterprise Linux
				</li><li class="listitem">
					SUSE
				</li><li class="listitem">
					Oracle Linux
				</li><li class="listitem">
					Alpine Linux
				</li><li class="listitem">
					Amazon Linux
				</li><li class="listitem">
					VMWare Photon
				</li><li class="listitem">
					Python
				</li></ul></div><section class="section" id="clair-intro"><div class="titlepage"><div><div><h2 class="title">6.1. Red Hat Quay vulnerability scanning using Clair</h2></div></div></div><p>
				Clair is equipped with three types of scanners, and a matcher and an updater:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Distribution Scanner</strong></span>: This scanner discovers <code class="literal">Distribution</code> information, which is typically the base operator system the layer demonstrates features of.
					</li><li class="listitem">
						<span class="strong strong"><strong>Package Scanner</strong></span>: This scanner performs a package scan on the selected layer and returns all of the found packages.
					</li><li class="listitem">
						<span class="strong strong"><strong>Repository Scanner</strong></span>: This scanner discovers any package repositories that are present in the layers.
					</li><li class="listitem">
						<span class="strong strong"><strong>Matcher</strong></span>: Matcher implementation is responsible for telling ClairCore which packages to query, how to query the security advisory database, and whether the discovered <code class="literal">Vulnerability</code> from the security advisory database affects the provided package.
					</li><li class="listitem">
						<span class="strong strong"><strong>Updater</strong></span>: The updater is responsible for fetching a security advisory database and parsing the contents.
					</li></ul></div><section class="section" id="clair-analyses"><div class="titlepage"><div><div><h3 class="title">6.1.1. Understanding Clair analyses</h3></div></div></div><p>
					Clair analyses can be broken down into three distinct parts:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Indexing</strong></span>: Indexing starts with submitting a <code class="literal">Manifest</code> to Clair. On receipt, Clair will fetch layers, scan their contents, and return an intermediate representation called an <code class="literal">IndexReport</code>.
						</p><p class="simpara">
							Manifests are Clair’s representation of a container image. Clair leverages the fact <code class="literal">OCI Manifests</code> and <code class="literal">Layers</code> are content-addressed to reduce duplicated work.
						</p><p class="simpara">
							Once a <code class="literal">Manifest</code> is indexed, the <code class="literal">IndexReport</code> is persisted for later retrieval.
						</p></li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Matching</strong></span>: Matching is taking an <code class="literal">IndexReport</code> and correlating vulnerabilities affecting the <code class="literal">Manifest</code> the report represents.
						</p><p class="simpara">
							Clair continuously ingests new security data and a request to the matcher will always provide users with the most up to date vulnerability analysis of an <code class="literal">IndexReport</code>.
						</p></li><li class="listitem">
							<span class="strong strong"><strong>Notifications</strong></span>: Clair implements a notification service. When new vulnerabilities are discovered, the notifier service will determine if these vulnerabilities affect any indexed <code class="literal">Manifests</code>. The notifier will then take action according to its configuration.
						</li></ul></div><section class="section" id="notifications_for_vulnerabilities_found_by_clair"><div class="titlepage"><div><div><h4 class="title">6.1.1.1. Notifications for vulnerabilities found by Clair</h4></div></div></div><p>
						Since Red Hat Quay 3.4, different notifications are triggered for various repository events. These notifications vary based on enabled features.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							This includes the event type <code class="literal">Package Vulnerability Found</code>
						</p></div></div><p>
						<code class="literal">Additional Filter</code> can be applied for <code class="literal">Security Level</code>, and there are various notification methods. Custom notification titles are also optional.
					</p></section></section><section class="section" id="clairv4-intro"><div class="titlepage"><div><div><h3 class="title">6.1.2. Clair v4</h3></div></div></div><p>
					Released with Red Hat Quay 3.4, Clair v4 is the latest version of Clair. It is built on a new architecture consisting of Clair Core and a service wrapper. Clair v4 made several enhancements to Clair v2, including:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Support for the Python programming language package. Support for additional languages is planned for future versions of Clair and Red Hat Quay.
						</li><li class="listitem">
							Immutable data model and a new manifest-oriented API.
						</li><li class="listitem">
							Refocus on the latest Open Container Initiative (OCI) specifications.
						</li><li class="listitem">
							Image hashes and layer hashes are now treated as content addressable, so that images are uniquely identified as a whole.
						</li></ul></div></section><section class="section" id="clairv4-arch"><div class="titlepage"><div><div><h3 class="title">6.1.3. Clair v4 architecture</h3></div></div></div><p>
					Clair v4 utilizes the ClairCore library as its engine for examining contents and reporting vulnerabilities. At a high level, you can consider Clair as a service wrapper to the functionality provided in the ClairCore library.
				</p><section class="section" id="claircore"><div class="titlepage"><div><div><h4 class="title">6.1.3.1. ClairCore</h4></div></div></div><p>
						ClairCore is the engine behind Clair v4’s container security solution. The ClairCore package exports domain models, interfaces that are necessary to plug into the business logic, and a default set of implementations. This default set of implementations defines the support matrix.
					</p><p>
						ClairCore relies on Postgres for its persistence and the library will handle migrations if configured to do so.
					</p><p>
						The diagram below is a high level overview of ClairCore’s architecture.
					</p><p>
						<span class="inlinemediaobject"><img src="images/clair-core-architecture.png" alt="Connection not secure"/></span>
					</p><p>
						When a <code class="literal">claircore.Manifest</code> is submitted to the LibIndex, the library will index its constituent parts and create a report with its findings.
					</p><p>
						When a <code class="literal">claircore.IndexReport</code> is provided to LibVuln, the library will discover vulnerabilities affecting it and generate a <code class="literal">claircore.Volunerability</code> report.
					</p><section class="section" id="clairv2-compare-v4"><div class="titlepage"><div><div><h5 class="title">6.1.3.1.1. Clair v2 and Clair v4 Comparison</h5></div></div></div><div class="table" id="idm45401777069984"><p class="title"><strong>Table 6.1. Clair v2 and Clair v4 component comparison</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 25%; " class="col_2"/><col style="width: 25%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45401777542592" scope="col">Component</th><th align="left" valign="top" id="idm45401777541376" scope="col">Clair v2</th><th align="left" valign="top" id="idm45401783440256" scope="col">Clair v4</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45401777542592">
										<p>
											API layers
										</p>
										</td><td align="left" valign="top" headers="idm45401777541376">
										<p>
											In Clair v2, clients were required to provide layers to the API.
										</p>
										</td><td align="left" valign="top" headers="idm45401783440256">
										<p>
											Clair v4 is manifest-based, providing an easier API for users.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45401777542592">
										<p>
											Insights and reports
										</p>
										</td><td align="left" valign="top" headers="idm45401777541376">
										<p>
											Clair v2 provided only insights on vulnerabilities
										</p>
										</td><td align="left" valign="top" headers="idm45401783440256">
										<p>
											Clair v4 provides detailed reports on the content of the container, which can be fed to other tools for analyses or inventory purposes.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45401777542592">
										<p>
											Architecture
										</p>
										</td><td align="left" valign="top" headers="idm45401777541376">
										<p>
											Clair v2 ran as a monolithic application.
										</p>
										</td><td align="left" valign="top" headers="idm45401783440256">
										<p>
											Clair v4 divides functionality across multiple services for ease of development and scaling use cases.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45401777542592">
										<p>
											Support for language packages
										</p>
										</td><td align="left" valign="top" headers="idm45401777541376">
										<p>
											Clair v2 does not support computer language packages.
										</p>
										</td><td align="left" valign="top" headers="idm45401783440256">
										<p>
											Clair v4 supports Python language packages, with plans of adding more in future versions.
										</p>
										</td></tr><tr><td align="left" valign="top" headers="idm45401777542592">
										<p>
											Package locator
										</p>
										</td><td align="left" valign="top" headers="idm45401777541376">
										<p>
											Clair v2 did not provide details on where packages were located inside of the container.
										</p>
										</td><td align="left" valign="top" headers="idm45401783440256">
										<p>
											Clair v4 identifies where packages are located inside of the container.
										</p>
										</td></tr></tbody></table></div></div></section></section></section><section class="section" id="clairv2-to-v4"><div class="titlepage"><div><div><h3 class="title">6.1.4. Migrating from Clair v2 to Clair v4</h3></div></div></div><p>
					Starting with Red Hat Quay 3.4, Clair v4 is used by default. It will also be the only version of Clair continually supported, as older versions of Red Hat Quay are not supported with Clair v4 in production. Users should continue using Clair v2 if using a version of Red Hat Quay earlier than 3.4.
				</p><p>
					Existing Red Hat Quay 3.3 deployments will be upgraded to Clair v4 when managed via the Red Hat Quay Operator. Manually upgraded Red Hat Quay deployments can install Clair v4 side-by-side, which will cause the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							All new image vulnerability scans to be performed by Clair v4
						</li><li class="listitem">
							Existing images to be rescanned by Clair v4
						</li></ul></div></section><section class="section" id="clairv4-limitations"><div class="titlepage"><div><div><h3 class="title">6.1.5. Clair v4 limitations</h3></div></div></div><p>
					The following limitations are currently being addressed by the development team:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							As of Clair v4, both operating system level and programming language packages are covered. The latter is currently limited to Python, however support for other languages will be added in the future.
						</li><li class="listitem">
							There is currently limited multi-arch support on Clair v4, which works for package managers like <code class="literal">rpm</code>, <code class="literal">yum</code>, and <code class="literal">dnf</code> that compensates for differences in endianess.
						</li><li class="listitem">
							Clair v4 does not currently support MSFT Windows images.
						</li><li class="listitem">
							Clair v4 does not currently support slim / scratch container images.
						</li></ul></div></section><section class="section" id="clairv4-air-gapped"><div class="titlepage"><div><div><h3 class="title">6.1.6. Air-gapped Clair v4</h3></div></div></div><p>
					Red Hat Quay 3.4 and later and Clair v4 are supported in disconnected environments. By default, Clair v4 will attempt to run automated updates against Red Hat servers. When Clair v4 in network environments is disconnected from the internet:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Clair v4 auto-update is disabled in the Clair <code class="literal">config</code> bundle.
						</li><li class="listitem">
							On a system with internet access, the vulnerability database updates is performed manually and exported to a disk.
						</li><li class="listitem">
							The on-disk data is then transferred to the target system with offline media. It is then manually imported.
						</li></ul></div><p>
					For more information on air-gapped Clair v4 and using <code class="literal">clairctl</code>, the command line tool, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/quay_operator_features#clair-openshift-airgap-update">Manually updating the vulnerability databases for Clair in an air-gapped OpenShift cluster</a>
				</p></section></section></section><section class="chapter" id="content-distrib-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Content distribution with Red Hat Quay</h1></div></div></div><p>
			Content distribution features in Red Hat Quay include:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="#mirroring-intro" title="7.1. Repository mirroring">Repository mirroring</a>
				</li><li class="listitem">
					<a class="link" href="#georepl-intro" title="7.2. Geo-replication">Geo-replication</a>
				</li><li class="listitem">
					<a class="link" href="#airgap-intro" title="7.4. Air-gapped / disconnected deployments">Deployment in air-gapped environments</a>
				</li></ul></div><section class="section" id="mirroring-intro"><div class="titlepage"><div><div><h2 class="title">7.1. Repository mirroring</h2></div></div></div><p>
				Red Hat Quay repository mirroring lets you mirror images from external container registries (or another local registry) into your Red Hat Quay cluster. Using repository mirroring, you can synchronize images to Red Hat Quay based on repository names and tags.
			</p><p>
				From your Red Hat Quay cluster with repository mirroring enabled, you can:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Choose a repository from an external registry to mirror
					</li><li class="listitem">
						Add credentials to access the external registry
					</li><li class="listitem">
						Identify specific container image repository names and tags to sync
					</li><li class="listitem">
						Set intervals at which a repository is synced
					</li><li class="listitem">
						Check the current state of synchronization
					</li></ul></div><p>
				To use the mirroring functionality, you need to:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Enable repository mirroring in the Red Hat Quay configuration
					</li><li class="listitem">
						Run a repository mirroring worker
					</li><li class="listitem">
						Create mirrored repositories
					</li></ul></div><p>
				All repository mirroring configuration can be performed using the configuration tool UI or via the Red Hat Quay API
			</p><section class="section" id="mirroring-using"><div class="titlepage"><div><div><h3 class="title">7.1.1. Using repository mirroring</h3></div></div></div><p>
					Here are some features and limitations of Red Hat Quay repository mirroring:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							With repository mirroring, you can mirror an entire repository or selectively limit which images are synced. Filters can be based on a comma-separated list of tags, a range of tags, or other means of identifying tags through regular expressions.
						</li><li class="listitem">
							Once a repository is set as mirrored, you cannot manually add other images to that repository.
						</li><li class="listitem">
							Because the mirrored repository is based on the repository and tags you set, it will hold only the content represented by the repo / tag pair. In other words, if you change the tag so that some images in the repository no longer match, those images will be deleted.
						</li><li class="listitem">
							Only the designated robot can push images to a mirrored repository, superseding any role-based access control permissions set on the repository.
						</li><li class="listitem">
							With a mirrored repository, a user can pull images (given read permission) from the repository but can not push images to the repository.
						</li><li class="listitem">
							Changing settings on your mirrored repository can be performed in the Red Hat Quay UI, using the Repositories → Mirrors tab for the mirrored repository you create.
						</li><li class="listitem">
							Images are synced at set intervals, but can also be synced on demand.
						</li></ul></div></section><section class="section" id="mirroring-recommend"><div class="titlepage"><div><div><h3 class="title">7.1.2. Repository mirroring recommendations</h3></div></div></div><p>
					Best practices for repository mirroring include:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Repository mirroring pods can run on any node. This means you can even run mirroring on nodes where Red Hat Quay is already running.
						</li><li class="listitem">
							Repository mirroring is scheduled in the database and runs in batches. As a result, more workers should mean faster mirroring, since more batches will be processed.
						</li><li class="listitem"><p class="simpara">
							The optimal number of mirroring pods depends on:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									The total number of repositories to be mirrored
								</li><li class="listitem">
									The number of images and tags in the repositories and the frequency of changes
								</li><li class="listitem">
									Parallel batches
								</li></ul></div></li><li class="listitem">
							You should balance your mirroring schedule across all mirrored repositories, so that they do not all start up at the same time.
						</li><li class="listitem">
							For a mid-size deployment, with approximately 1000 users and 1000 repositories, and with roughly 100 mirrored repositories, it is expected that you would use 3-5 mirroring pods, scaling up to 10 pods if required.
						</li></ul></div></section><section class="section" id="mirroring-events"><div class="titlepage"><div><div><h3 class="title">7.1.3. Event notifications for mirroring</h3></div></div></div><p>
					There are three notification events for repository mirroring:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Repository Mirror Started
						</li><li class="listitem">
							Repository Mirror Success
						</li><li class="listitem">
							Repository Mirror Unsuccessful
						</li></ul></div><p>
					The events can be configured inside the Settings tab for each repository, and all existing notification methods such as email, slack, Quay UI and webhooks are supported.
				</p></section><section class="section" id="mirroring-api-intro"><div class="titlepage"><div><div><h3 class="title">7.1.4. Mirroring API</h3></div></div></div><p>
					You can use the Red Hat Quay API to configure repository mirroring:
				</p><p>
					<span class="inlinemediaobject"><img src="images/swagger-mirroring.png" alt="Mirroring API"/></span>
				</p><p>
					More information is available in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/red_hat_quay_api_guide/index">Red Hat Quay API Guide</a>
				</p></section></section><section class="section" id="georepl-intro"><div class="titlepage"><div><div><h2 class="title">7.2. Geo-replication</h2></div></div></div><p>
				Geo-replication allows multiple, geographically distributed Red Hat Quay deployments to work as a single registry from the perspective of a client or user. It significantly improves push and pull performance in a globally-distributed Red Hat Quay setup. Image data is asynchronously replicated in the background with transparent failover / redirect for clients.
			</p><p>
				With Red Hat Quay 3.7, deployments of Red Hat Quay with geo-replication is supported by standalone and Operator deployments.
			</p><section class="section" id="geo_replication_features"><div class="titlepage"><div><div><h3 class="title">7.2.1. Geo-replication features</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When geo-replication is configured, container image pushes will be written to the preferred storage engine for that Red Hat Quay instance (typically the nearest storage backend within the region).
						</li><li class="listitem">
							After the initial push, image data will be replicated in the background to other storage engines.
						</li><li class="listitem">
							The list of replication locations is configurable and those can be different storage backends.
						</li><li class="listitem">
							An image pull will always use the closest available storage engine, to maximize pull performance.
						</li><li class="listitem">
							If replication hasn’t been completed yet, the pull will use the source storage backend instead.
						</li></ul></div></section><section class="section" id="georepl-prereqs"><div class="titlepage"><div><div><h3 class="title">7.2.2. Geo-replication requirements and constraints</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A single database, and therefore all metadata and Quay configuration, is shared across all regions.
						</li><li class="listitem">
							A single Redis cache is shared across the entire Quay setup and needs to accessible by all Quay pods.
						</li><li class="listitem">
							The exact same configuration should be used across all regions, with exception of the storage backend, which can be configured explicitly using the <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environment variable.
						</li><li class="listitem">
							Geo-Replication requires object storage in each region. It does not work with local storage or NFS.
						</li><li class="listitem">
							Each region must be able to access every storage engine in each region (requires a network path).
						</li><li class="listitem">
							Alternatively, the storage proxy option can be used.
						</li><li class="listitem">
							The entire storage backend (all blobs) is replicated. This is in contrast to repository mirroring, which can be limited to an organization or repository or image.
						</li><li class="listitem">
							All Quay instances must share the same entrypoint, typically via load balancer.
						</li><li class="listitem">
							All Quay instances must have the same set of superusers, as they are defined inside the common configuration file.
						</li><li class="listitem">
							Geo-replication requires your Clair configuration to be set to <code class="literal">unmanaged</code>. An unmanaged Clair database allows the Red Hat Quay Operator to work in a geo-replicated environment, where multiple instances of the Operator must communicate with the same database. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html-single/deploy_red_hat_quay_on_openshift_with_the_quay_operator/index#clair-unmanaged">Advanced Clair configuration</a>.
						</li><li class="listitem">
							Geo-Replication requires SSL/TSL certificates and keys. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.7/html-single/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/index#using_ssl_to_protect_connections_to_red_hat_quay">Using SSL to protect connections to Red Hat Quay</a>.
						</li></ul></div><p>
					If the above requirements cannot be met, you should instead use two or more distinct Quay deployments and take advantage of repository mirroring functionality.
				</p></section><section class="section" id="georepl-arch-standalone"><div class="titlepage"><div><div><h3 class="title">7.2.3. Geo-replication using standalone Red Hat Quay</h3></div></div></div><p>
					<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_georeplication.png" alt="Georeplication"/></span>
				</p><p>
					In the example shown above, Quay is running standalone in two separate regions, with a common database and a common Redis instance. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Quay instance, and will then be replicated, in the background, to the other storage engines.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In the event that Clair fails in one cluster, for example, the US cluster, US users would not see vulnerability reports in Quay for the second cluster (EU). This is because all Clair instances have the same state. When Clair fails, it is usually because of a problem within the cluster.
					</p></div></div></section><section class="section" id="georepl-arch-operator"><div class="titlepage"><div><div><h3 class="title">7.2.4. Geo-replication using the Red Hat Quay Operator</h3></div></div></div><p>
					<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_georeplication_openshift-temp.png" alt="Georeplication architecture"/></span>
				</p><p>
					In the example shown above, the Red Hat Quay Operator is deployed in two separate regions, with a common database and a common Redis instance. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Quay instance, and will then be replicated, in the background, to the other storage engines.
				</p><p>
					Because the Operator now manages the Clair security scanner and its database separately, geo-replication setups can be leveraged so that they do not manage the Clair database. Instead, an external shared database would be used. Red Hat Quay and Clair support several providers and vendors of PostgreSQL, which can be found in the Red Hat Quay 3.x <a class="link" href="https://access.redhat.com/articles/4067991">test matrix</a>. Additionally, the Operator also supports custom Clair configurations that can be injected into the deployment, which allows users to configure Clair with the connection credentials for the external database.
				</p></section><section class="section" id="georepl-mixed-storage"><div class="titlepage"><div><div><h3 class="title">7.2.5. Mixed storage for geo-replication</h3></div></div></div><p>
					Red Hat Quay geo-replication supports the use of different and multiple replication targets, for example, using AWS S3 storage on public cloud and using Ceph storage on-prem. This complicates the key requirement of granting access to all storage backends from all Red Hat Quay pods and cluster nodes. As a result, it is recommended that you:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use a VPN to prevent visibility of the internal storage <span class="emphasis"><em>or</em></span>
						</li><li class="listitem">
							Use a token pair that only allows access to the specified bucket used by Quay
						</li></ul></div><p>
					This will result in the public cloud instance of Red Hat Quay having access to on-prem storage but the network will be encrypted, protected, and will use ACLs, thereby meeting security requirements.
				</p><p>
					If you cannot implement these security measures, it may be preferable to deploy two distinct Red Hat Quay registries and to use repository mirroring as an alternative to geo-replication.
				</p></section></section><section class="section" id="mirroring-versus-georepl"><div class="titlepage"><div><div><h2 class="title">7.3. Repository mirroring versus geo-replication</h2></div></div></div><p>
				Red Hat Quay geo-replication mirrors the entire image storage backend data between 2 or more different storage backends while the database is shared (one Red Hat Quay registry with two different blob storage endpoints). The primary use cases for geo-replication are:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Speeding up access to the binary blobs for geographically dispersed setups
					</li><li class="listitem">
						Guaranteeing that the image content is the same across regions
					</li></ul></div><p>
				Repository mirroring synchronizes selected repositories (or subsets of repositories) from one registry to another. The registries are distinct, with each registry having a separate database and separate image storage. The primary use cases for mirroring are:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Independent registry deployments in different datacenters or regions, where a certain subset of the overall content is supposed to be shared across the datacenters / regions
					</li><li class="listitem">
						Automatic synchronization or mirroring of selected (whitelisted) upstream repositories from external registries into a local Red Hat Quay deployment
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Repository mirroring and geo-replication can be used simultaneously.
				</p></div></div><div class="table" id="idm45401777059248"><p class="title"><strong>Table 7.1. Red Hat Quay Repository mirroring versus geo-replication</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"/><col style="width: 33%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm45401780517328" scope="col">Feature / Capability</th><th align="left" valign="top" id="idm45401778818368" scope="col">Geo-replication</th><th align="left" valign="top" id="idm45401778817504" scope="col">Repository mirroring</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								What is the feature designed to do?
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								A shared, global registry
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								Distinct, different registries
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								What happens if replication or mirroring hasn’t been completed yet?
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								The remote copy is used (slower)
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								No image is served
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								Is access to all storage backends in both regions required?
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								Yes (all Red Hat Quay nodes)
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								No (distinct storage)
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								Can users push images from both sites to the same repository?
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								No
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								Is all registry content and configuration identical across all regions (shared database)
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								No
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								Can users select individual namespaces or repositories to be mirrored?
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								Yes
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								Can users apply filters to synchronization rules?
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								Yes
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401780517328">
							<p>
								Are individual / different RBAC configurations allowed in each region
							</p>
							</td><td align="left" valign="top" headers="idm45401778818368">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm45401778817504">
							<p>
								Yes
							</p>
							</td></tr></tbody></table></div></div></section><section class="section" id="airgap-intro"><div class="titlepage"><div><div><h2 class="title">7.4. Air-gapped / disconnected deployments</h2></div></div></div><p>
				The following diagram shows how Red Hat Quay and Clair can be deployed in air-gapped or disconnected environments:
			</p><p>
				<span class="inlinemediaobject"><img src="images/178_Quay_architecture_0821_air-gapped.png" alt="Air-gapped deployment"/></span>
			</p><p>
				The upper deployment in the diagram shows Red Hat Quay and Clair connected to the internet, with an air-gapped OpenShift cluster accessing the Quay registry through an explicit, white-listed hole in the firewall.
			</p><p>
				The lower deployment in the diagram shows Red Hat Quay and Clair running inside the firewall, with image and CVE data transferred to the target system using offline media. The data is exported from a separate Quay and Clair deployment that is connected to the internet.
			</p><section class="section" id="airgap-clair"><div class="titlepage"><div><div><h3 class="title">7.4.1. Using Clair in air-gapped environments</h3></div></div></div><p>
					By default, Clair will attempt to run automated updates against Red Hat servers. To run Clair in network environments that are disconnected from the internet:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Disable Clair auto-update in the Clair configuration bundle.
						</li><li class="listitem">
							Manually update the vulnerability database on a system with internet access and then export to disk.
						</li><li class="listitem">
							Transfer the on-disk data to the target system using offline media and then manually import it into Clair.
						</li></ul></div><p>
					Using Clair in air-gapped environments is fully containerized and, as a result, is easy to automate.
				</p></section></section></section><section class="chapter" id="sizing-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Red Hat Quay sizing and subscriptions</h1></div></div></div><p>
			Scalability of Red Hat Quay is one of its key strengths, with a single code base supporting a broad spectrum of deployment sizes, including:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Proof of concept deployment on single development machine
				</li><li class="listitem">
					Mid-size deployment of approximately 2,000 users that can serve content to dozens of Kubernetes clusters
				</li><li class="listitem">
					High-end deployment such as <code class="literal">Quay.io</code> that can server thousands of Kubernetes clusters world-wide
				</li></ul></div><p>
			Since sizing heavily depends on a multitude of factors, such as the number of users, images, concurrent pulls and pushes, there are no standard sizing recommendations.
		</p><p>
			The following are the minimum requirements for systems running Red Hat Quay (per container/pod instance):
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<span class="strong strong"><strong>Quay:</strong></span> minimum 6GB; recommended 8GB + swap, 2 more more vCPUs
				</li><li class="listitem">
					<span class="strong strong"><strong>Clair:</strong></span> recommended 2GB RAM + sqap, 2 or more vCPUs
				</li><li class="listitem">
					<span class="strong strong"><strong>Storage:</strong></span>: recommended 30GB
				</li><li class="listitem">
					<span class="strong strong"><strong>NooBaa:</strong></span> minimum 2GB, 1 vCPU (when <code class="literal">objectstorage</code> component is selected via the Operator)
				</li><li class="listitem">
					<span class="strong strong"><strong>Clair database:</strong></span> minimum 200MB required for security metadata
				</li></ul></div><p>
			Stateless components of Red Hat Quay can be scaled out but this will cause a heavier load on stateful backend services.
		</p><section class="section" id="sizing-sample"><div class="titlepage"><div><div><h2 class="title">8.1. Red Hat Quay sample sizings</h2></div></div></div><p>
				The following table shows typical sizing for three deployment sizes: proof of concept, mid-size, and high-end. Whether a deployment runs appropriately with the same metrics will depend on many other factors not shown below.
			</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"/><col style="width: 20%; " class="col_2"/><col style="width: 20%; " class="col_3"/><col style="width: 20%; " class="col_4"/></colgroup><thead><tr><th align="left" valign="top" id="idm45401777518448" scope="col">Metric</th><th align="center" valign="top" id="idm45401777517360" scope="col">Proof of concept</th><th align="center" valign="top" id="idm45401777516272" scope="col">Mid-size</th><th align="center" valign="top" id="idm45401777507520" scope="col">High End<br/> (Quay.io)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								No. of Quay containers by default
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								1
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								4
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								15
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								No. of Quay containers max at scale-out
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								N/A
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								8
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								30
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								No. of Clair containers by default
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								1
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								3
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								10
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								No. of Clair containers max at scale-out
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								N/A
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								6
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								15
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								No. of mirroring pods (to mirror 100 repos)
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								1
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								5-10
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								N/A
							</p>
							</td></tr><tr><td align="left" valign="middle" headers="idm45401777518448">
							<p>
								Database sizing
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								2 -4 Cores<br/> 6-8 GB RAM<br/> 10-20GB disk
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								4-8 Cores<br/> 6-32 GB RAM<br/> 100GB-1TB disk
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								32 cores<br/> 244GB<br/> 1+ TB disk
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								Object storage backend sizing
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								10-100 GB
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								1 - 20 TB
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								50+ TB up to PB
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								Redis cache sizing
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360"> </td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								2 Cores<br/> 2-4 GB RAM
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								4 cores <br/> 28 GB RAM
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45401777518448">
							<p>
								Underlying node sizing<br/> (physical or virtual)
							</p>
							</td><td align="center" valign="top" headers="idm45401777517360">
							<p>
								4 Cores<br/> 8 GB RAM
							</p>
							</td><td align="center" valign="top" headers="idm45401777516272">
							<p>
								4-6 Cores<br/> 12-16 GB RAM
							</p>
							</td><td align="center" valign="top" headers="idm45401777507520">
							<p>
								Quay:<br/> 13 cores<br/> 56GB RAM<br/><br/> Clair:<br/> 2 cores<br/> 4 GB RAM
							</p>
							</td></tr></tbody></table></div><p>
				For further details on sizing &amp; related recommendations for mirroring, see the section on <a class="link" href="#mirroring-intro" title="7.1. Repository mirroring">repository mirroring</a>.
			</p><p>
				The sizing for the Redis cache is only relevant if you use Quay builders, otherwise it is not significant.
			</p></section><section class="section" id="subscription-intro"><div class="titlepage"><div><div><h2 class="title">8.2. Red Hat Quay subscription information</h2></div></div></div><p>
				Red Hat Quay is available with Standard or Premium support, and subscriptions are based on deployments.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Deployment means an installation of a single Red Hat Quay registry using a shared data backend.
				</p></div></div><p>
				With a Red Hat Quay subscription:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						There is no limit on the number of pods (Quay, Clair, Builder, etc.) you can deploy.
					</li><li class="listitem">
						Red Hat Quay pods can run in multiple data centers or availability zones.
					</li><li class="listitem">
						Storage and database backends can be deployed across multiple data centers or availability zones, but only as a single, shared storage backend and single, shared database backend.
					</li><li class="listitem">
						Red Hat Quay can manage content for an unlimited number of clusters or standalone servers.
					</li><li class="listitem">
						Clients can access the Quay deployment irrespective of their physical location.
					</li><li class="listitem">
						You can deploy Red Hat Quay on OpenShift infrastructure nodes to minimize subscription requirements.
					</li><li class="listitem">
						You can run the Container Security Operator (CSO) and the Quay Bridge Operator (QBO) on your OpenShift clusters at no additional cost.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Red Hat Quay geo-replication requires a subscription for each storage replication. The database, however, is shared.
				</p></div></div><p>
				For more information on purchasing a Red Hat Quay subscription, see <a class="link" href="https://www.redhat.com/en/technologies/cloud-computing/quay">Red Hat Quay</a>.
			</p></section><section class="section" id="quay-internal-registry-intro"><div class="titlepage"><div><div><h2 class="title">8.3. Using Red Hat Quay with or without internal registry</h2></div></div></div><p>
				Red Hat Quay can be used as an external registry in front of multiple OpenShift clusters with their internal registries.
			</p><p>
				Red Hat Quay can also be used in place of the internal registry when it comes to automating builds and deployment rollouts. The required coordination of <code class="literal">Secrets</code> and <code class="literal">ImageStreams</code> is automated by the Quay Bridge Operator, which can be launched from the OperatorHub for OpenShift.
			</p></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm45401782315840"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2022 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>